{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "## Decision Tree Classifier using Spark Mllib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Used: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29/wdbc.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "#sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Dc\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data as Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 569\n"
     ]
    }
   ],
   "source": [
    "raw_data = sc.textFile('New_Data.csv')\n",
    "print(\"Train data size is {}\".format(raw_data.count()))\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Program Code\n",
    "\n",
    "### Preparing the data RDD to pass into Decision tree classifier :: Converting class into numeric from characters(M,B)\n",
    "### Splitting the cleaned data into 80:20 ratio of Training and Testing\n",
    "\n",
    "##### Please note that only the last 10 features are considered for model building : look into section(c) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41]\n",
    "    clean_line_split = line_split[0:10]\n",
    "    class_ =line_split[10]\n",
    "    return LabeledPoint(class_, array([float(x) for x in clean_line_split]))\n",
    "data = csv_data.map(create_labeled_point)\n",
    "\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2],seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([LabeledPoint(1.0, [4.0,0.0,4.0,3.0,3.0,3.0,3.0,5.0,3.0,3.0]),\n",
       "  LabeledPoint(1.0, [4.0,1.0,3.0,2.0,2.0,0.0,1.0,4.0,1.0,1.0])],\n",
       " PythonRDD[71] at RDD at PythonRDD.scala:52)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2),trainingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTree.trainClassifier(data, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={},\n",
    "                                          impurity='entropy', maxDepth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Class values of the Test data and Calculating the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.061946902654867256\n"
     ]
    }
   ],
   "source": [
    "predictions = tree_model.predict(testData.map(lambda p: p.features))\n",
    "labels_and_preds = testData.map(lambda p: p.label).zip(predictions)\n",
    "testErr = labels_and_preds.filter(\n",
    "    lambda p: p[0] != p[1]).count() / float(testData.count())\n",
    "\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please note that additional performance metrics are presented in the section (f) , of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) The choice of parameters :\n",
    "* #### Impurity or Attribute Selection Method = \"Gini\"\n",
    "* #### MaxDepth = 3 (Given)\n",
    "* #### Max Bins = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Notes and Any assumptions made :\n",
    "\n",
    "### * Understanding the columns :\n",
    "* The 10 features radius,texture,perimeter,area,smoothness,compactness,concavity,concave points,symmetry,fractal_dimension            are presented in 3 dimensions : Mean, Standard error and worst sequentially.\n",
    "* Mean is the mean of all the cells, Standard error is the SD of all the cells and worst is the mean of 3 largest mean values.\n",
    "### * Assumptions :\n",
    "* The first feature ID is not contributing to the model hence ignored.\n",
    "* The features captured as Worst measure represent the data better than just measure and Standard error. Hence I have used only the columns from 22 till the end as my feature set.: radius_worst,texture_worst,perimeter_worst,area_worst,smoothness_worst\tcompactness_worst,concavity_worst,concave points_worst,symmetry_worstfractal_dimension_worst.\n",
    "* I have validated the model on the other two sets (Mean and Standard Deviation) and found a better performance when I passed the worst_dimension features. You can look at the last section for the other runs.\n",
    "* The test errors are ::\n",
    "Mean : 0.097, SD : 0.141, Worst_ = 0.035( Smallest and hence the best gives the best features for model building)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Validation and Train/Test Startegy Used:\n",
    "\n",
    " ### Used the k-fold cross validation to evaluate the skill of the decision tree algorithm being learnt in general. I used the value k = 5 and the 4th fold seemed to be the best split giving out the accuracy upto 96.4%\n",
    " ### Used the criteria and split of that model in my program as parameters.\n",
    " ### max_depth is fixed, min_samples_leaf as minInstancesPerNode and criteria: gini\n",
    " \n",
    " ## Below is the code for Cross Validation employed. Ran on the feature_worst Measures only(Last 10 feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = 'C:\\\\Users\\\\yandr\\\\OneDrive\\\\Desktop\\\\BigData\\\\spark\\\\Data_Input.csv'\n",
    "\n",
    "df1 = pd.read_csv(File)\n",
    "\n",
    "Train,Test = train_test_split(df1, test_size=0.2)\n",
    "Data_X = Train.values[:,20:31]\n",
    "Data_Y = Train.values[:,31]\n",
    "\n",
    "X_Test=Test.values[:,20:31]\n",
    "Y_Test=Test.values[:,31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,random_state=None, shuffle=True)\n",
    "tree_fold = []\n",
    "acc_tree=[]\n",
    "prec_tree=[]\n",
    "rec_tree=[]\n",
    "\n",
    "def train_tree(X_train,X_test,Y_train,Y_test):\n",
    "   tree = DecisionTreeClassifier(criterion = \"gini\",max_depth=3,min_samples_leaf=3,random_state = 200)\n",
    "   tree.fit(X_train, Y_train)\n",
    "   pred=tree.predict(X_test)\n",
    "   tree_fold.append(tree)\n",
    "   acc_tree.append(accuracy_score(Y_test,pred))\n",
    "   prec_tree.append(precision_score(Y_test,pred,average= 'macro'))\n",
    "   rec_tree.append(recall_score(Y_test,pred,average= 'macro')) \n",
    "   return\n",
    "\n",
    "for train_index, test_index in kf.split(Data_X):\n",
    "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "  X_train, X_test = Data_X[train_index], Data_X[test_index]\n",
    "  Y_train, Y_test = Data_Y[train_index], Data_Y[test_index]\n",
    "\n",
    "  train_tree(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge of the metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9340659340659341,\n",
       "  0.945054945054945,\n",
       "  0.9120879120879121,\n",
       "  0.945054945054945,\n",
       "  0.9555555555555556],\n",
       " [0.9274274274274275,\n",
       "  0.9206349206349206,\n",
       "  0.9102564102564104,\n",
       "  0.9413236929922135,\n",
       "  0.9583333333333333],\n",
       " [0.9357142857142857,\n",
       "  0.9496969696969697,\n",
       "  0.9102564102564104,\n",
       "  0.9336065573770491,\n",
       "  0.9509109311740891])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_tree,prec_tree,rec_tree\n",
    "#Calculating the average Performances:\n",
    "Avg_acc_tree= sum(acc_tree)/len(acc_tree)\n",
    "Avg_prec_tree= sum(prec_tree)/len(prec_tree)\n",
    "Avg_rec_tree= sum(rec_tree)/len(rec_tree)\n",
    "\n",
    "print(\"Averge of the metrics:\")\n",
    "\n",
    "Avg_acc_tree,Avg_prec_tree,Avg_rec_tree\n",
    "acc_tree,prec_tree,rec_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Using the best Fold:\n",
      "Accuracy = 0.9649122807017544 Precision = [0.97142857 0.95454545] Recall = [0.97142857 0.95454545]\n",
      "Confusion Matrix =\n",
      "[[68  2]\n",
      " [ 2 42]]\n"
     ]
    }
   ],
   "source": [
    "pred_tree=tree_fold[3].predict(X_Test)\n",
    "c_tree=confusion_matrix(Y_Test, pred_tree, labels=None, sample_weight=None)\n",
    "acc_tree=accuracy_score(Y_Test,pred_tree)\n",
    "prec_tree=precision_score(Y_Test,pred_tree,average= None)\n",
    "rec_tree=recall_score(Y_Test,pred_tree,average= None)\n",
    "print(\"Performance Using the best Fold:\")\n",
    "\n",
    "print(\"Accuracy =\",acc_tree,\"Precision =\",prec_tree,\"Recall =\" ,rec_tree)\n",
    "\n",
    "print(\"Confusion Matrix =\")\n",
    "print(c_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Decision tree Obtained: model is built in the pyspark. Look above for section a) for code\n",
    "\n",
    "####  * Please note that the features used are the last 10 features (Worst measure) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 13 nodes\n",
      "  If (feature 2 <= 103.0)\n",
      "   If (feature 7 <= 0.1972)\n",
      "    If (feature 3 <= 754.1500000000001)\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 > 754.1500000000001)\n",
      "     Predict: 0.0\n",
      "   Else (feature 7 > 0.1972)\n",
      "    Predict: 1.0\n",
      "  Else (feature 2 > 103.0)\n",
      "   If (feature 2 <= 119.6)\n",
      "    If (feature 4 <= 0.13585)\n",
      "     Predict: 0.0\n",
      "    Else (feature 4 > 0.13585)\n",
      "     Predict: 1.0\n",
      "   Else (feature 2 > 119.6)\n",
      "    If (feature 6 <= 0.18845)\n",
      "     Predict: 0.0\n",
      "    Else (feature 6 > 0.18845)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Performance shown by the confusion matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics=MulticlassMetrics(labels_and_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.9380530973451328\n",
      "Precision = 0.9380530973451328\n",
      "Accuracy = 0.9380530973451328\n",
      "[[69.  4.]\n",
      " [ 3. 37.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall = %s\" % metrics.recall())\n",
    "print(\"Precision = %s\" % metrics.precision())\n",
    "#print(\"F1 measure = %s\" % metrics.f1Measure())\n",
    "print(\"Accuracy = %s\" % metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Runs : Validating the assumptions:\n",
    "\n",
    "## Model for Standard deviation features: radius_mean\ttexture_mean\tperimeter_mean\tarea_mean\tsmoothness_mean\tcompactness_mean\tconcavity_mean\tconcave points_mean\tsymmetry_mean\tfractal_dimension_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41]\n",
    "    clean_line_split = line_split[1:11]\n",
    "    class_ = 1.0\n",
    "    if line_split[31] == '0':\n",
    "        class_ = 0.0\n",
    "    return LabeledPoint(class_, array([float(x) for x in clean_line_split]))\n",
    "data = csv_data.map(create_labeled_point)\n",
    "\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2],seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTree.trainClassifier(trainingData, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={},\n",
    "                                          impurity='gini', maxDepth=3,maxBins=7,minInstancesPerNode=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.09734513274336283\n"
     ]
    }
   ],
   "source": [
    "predictions = tree_model.predict(testData.map(lambda p: p.features))\n",
    "labels_and_preds = testData.map(lambda p: p.label).zip(predictions)\n",
    "testErr = labels_and_preds.filter(\n",
    "    lambda p: p[0] != p[1]).count() / float(testData.count())\n",
    "\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Standard deviation features : radius_se\ttexture_se\tperimeter_se\tarea_se\tsmoothness_se\tcompactness_se\tconcavity_se\tconcave points_se\tsymmetry_se\tfractal_dimension_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41]\n",
    "    clean_line_split = line_split[11:21]\n",
    "    class_ = 1.0\n",
    "    if line_split[31] == '0':\n",
    "        class_ = 0.0\n",
    "    return LabeledPoint(class_, array([float(x) for x in clean_line_split]))\n",
    "data = csv_data.map(create_labeled_point)\n",
    "\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2],seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTree.trainClassifier(trainingData, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={},\n",
    "                                          impurity='gini', maxDepth=3,maxBins=7,minInstancesPerNode=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.1415929203539823\n"
     ]
    }
   ],
   "source": [
    "predictions = tree_model.predict(testData.map(lambda p: p.features))\n",
    "labels_and_preds = testData.map(lambda p: p.label).zip(predictions)\n",
    "testErr = labels_and_preds.filter(\n",
    "    lambda p: p[0] != p[1]).count() / float(testData.count())\n",
    "\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956\n",
    "\n",
    "https://spark.apache.org/docs/2.3.1/mllib-decision-tree\n",
    "\n",
    "https://gite.lirmm.fr/yagoubi/spark/commit/fdb302f49c021227026909bdcdade7496059013f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
